{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import glob, os\n",
    "# glob.glob(os.path.join(os.environ[\"SPARK_HOME\"], \"conf\", \"spark*\"))\n",
    "# print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "\n",
    "testdf = pd.read_csv('datasets/test.csv')\n",
    "# traindf = pd.read_csv('datasets/train.csv')\n",
    "alldf = pd.read_csv('datasets/submit.csv')\n",
    "\n",
    "# print(testdf.shape, traindf.shape)\n",
    "\n",
    "# print('cleaning dataframes')\n",
    "\n",
    "# print(np.sum(testdf.isnull()))\n",
    "\n",
    "testdf[['title', 'author']] = testdf[['title', 'author']].fillna(' ')\n",
    "\n",
    "testdf = testdf.dropna()\n",
    "# traindf = traindf.dropna()\n",
    "\n",
    "# print(testdf.shape, traindf.shape)\n",
    "\n",
    "ytestdf = testdf.drop(['title', 'author', 'text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5193,)\n"
     ]
    }
   ],
   "source": [
    "ytestdf = ytestdf.merge(alldf, left_on='id', right_on='id', how='left')[['id', 'label']]\n",
    "# ytestdf = pd.merge(left = ytestdf, right=alldf, left_on='id', right_on='id', how='left')\n",
    "ytest = ytestdf.label\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://d-i89-236-125.student.eduroam.uq.edu.au:4040\n",
      "[('spark.app.id', 'local-1648610385004'), ('spark.app.name', 'fakenews'), ('spark.sql.warehouse.dir', 'file:/Users/rushi/Documents/GitHub/fake-news-detection/spark-warehouse'), ('spark.driver.memory', '10g'), ('spark.executor.id', 'driver'), ('spark.driver.port', '65156'), ('spark.app.startTime', '1648610384967'), ('spark.driver.host', 'd-i89-236-125.student.eduroam.uq.edu.au'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]\n",
      "<pyspark.sql.session.SparkSession object at 0x7f84e0bb8ac0>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    master('local[*]').\\\n",
    "        config('spark.driver.memory', '10g').\\\n",
    "            appName('fakenews').\\\n",
    "                getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.uiWebUrl)\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "    # master('local[*]').\\\n",
    "    #     config('spark.driver.memory','10g').\\\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12256 5194\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "trainschema = StructType([StructField('id', IntegerType()),\\\n",
    "    StructField('title', StringType()),\\\n",
    "        StructField('author', StringType()),\\\n",
    "            StructField('text', StringType()),\\\n",
    "                StructField('label', IntegerType())]) \n",
    "\n",
    "testschema = StructType([StructField('id', IntegerType()),\\\n",
    "    StructField('title', StringType()),\\\n",
    "        StructField('author', StringType()),\\\n",
    "            StructField('text', StringType())]) \n",
    "\n",
    "trainset = spark.read.csv('datasets/train.csv', header=True, schema=trainschema)\n",
    "testset = spark.read.csv('datasets/test.csv', header=True, schema=testschema)\n",
    "allset = spark.read.csv('datasets/submit.csv', header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "trainset = trainset.fillna(' ', subset=['title', 'author']).dropna(how='any')\n",
    "testset = testset.fillna(' ', subset=['title', 'author']).dropna(how='any')\n",
    "\n",
    "print(trainset.count(), testset.count())\n",
    "\n",
    "# trainset.printSchema()\n",
    "# testset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset.show(10)\n",
    "\n",
    "testset = testset.join(allset, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, CountVectorizer, StopWordsRemover, IDF\n",
    "from pyspark.sql.functions import regexp_replace, udf, col, size\n",
    "\n",
    "#remove punctuation and remove text with less than 10 words\n",
    "def lower_clean_str(x):\n",
    "    punc = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~1234567890'\n",
    "    lowercased_str = x.lower()\n",
    "    for ch in punc:\n",
    "        lowercased_str = lowercased_str.replace(ch,'')\n",
    "    return lowercased_str\n",
    "\n",
    "udfclean = udf(lower_clean_str, StringType())\n",
    "\n",
    "trainset = trainset.withColumn('cleaned', udfclean(trainset.text)).dropna()\n",
    "testset = testset.withColumn('cleaned', udfclean(testset.text)).dropna()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='cleaned', outputCol='words')\n",
    "\n",
    "trainset = tokenizer.transform(trainset)\n",
    "testset = tokenizer.transform(testset)\n",
    "\n",
    "trainset = trainset.where(size(col('words')) > 10)\n",
    "testset = testset.where(size(col('words')) > 10)\n",
    "\n",
    "testsetdf = testset.toPandas()\n",
    "\n",
    "# regex = '[,\\\\-]'\n",
    "\n",
    "# trainset = trainset.withColumn('text', regexp_replace(trainset.text, regex, ' '))\n",
    "# testset = testset.withColumn('text', regexp_replace(testset.text, regex, ' '))\n",
    "\n",
    "#tonkenize text\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol='cleaned', outputCol='words')\n",
    "# wordsdata = tokenizer.transform(trainset)\n",
    "\n",
    "stopwr = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
    "\n",
    "hashing = HashingTF(inputCol='filtered', outputCol='hashed', numFeatures=700)\n",
    "\n",
    "hashingIDF = IDF(inputCol='hashed', outputCol='features')\n",
    "\n",
    "#create a model\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(maxIter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
       "      <td>0</td>\n",
       "      <td>palo alto calif  —   after years of scorning t...</td>\n",
       "      <td>[palo, alto, calif, , —, , , after, years, of,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>nan</td>\n",
       "      <td>\"Russian warships ready to strike terrorists n...</td>\n",
       "      <td>1</td>\n",
       "      <td>russian warships ready to strike terrorists ne...</td>\n",
       "      <td>[russian, warships, ready, to, strike, terrori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
       "      <td>0</td>\n",
       "      <td>videos nodapl native american leaders vow to s...</td>\n",
       "      <td>[videos, nodapl, native, american, leaders, vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
       "      <td>Daniel Victor</td>\n",
       "      <td>If at first you don’t succeed, try a different...</td>\n",
       "      <td>1</td>\n",
       "      <td>if at first you don’t succeed try a different ...</td>\n",
       "      <td>[if, at, first, you, don’t, succeed, try, a, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report: Meme Wars (E995)</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
       "      <td>1</td>\n",
       "      <td>mins ago  views  comments  likes for the firs...</td>\n",
       "      <td>[, mins, ago, , views, , comments, , likes, fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues, if Not Purse...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  #NoDAPL: Native American Leaders Vow to Stay A...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback, This ...   \n",
       "4  20804                    Keiser Report: Meme Wars (E995)   \n",
       "\n",
       "                    author                                               text  \\\n",
       "0         David Streitfeld  PALO ALTO, Calif.  —   After years of scorning...   \n",
       "1                      nan  \"Russian warships ready to strike terrorists n...   \n",
       "2            Common Dreams  Videos #NoDAPL: Native American Leaders Vow to...   \n",
       "3            Daniel Victor  If at first you don’t succeed, try a different...   \n",
       "4  Truth Broadcast Network  42 mins ago 1 Views 0 Comments 0 Likes 'For th...   \n",
       "\n",
       "   label                                            cleaned  \\\n",
       "0      0  palo alto calif  —   after years of scorning t...   \n",
       "1      1  russian warships ready to strike terrorists ne...   \n",
       "2      0  videos nodapl native american leaders vow to s...   \n",
       "3      1  if at first you don’t succeed try a different ...   \n",
       "4      1   mins ago  views  comments  likes for the firs...   \n",
       "\n",
       "                                               words  \n",
       "0  [palo, alto, calif, , —, , , after, years, of,...  \n",
       "1  [russian, warships, ready, to, strike, terrori...  \n",
       "2  [videos, nodapl, native, american, leaders, vo...  \n",
       "3  [if, at, first, you, don’t, succeed, try, a, d...  \n",
       "4  [, mins, ago, , views, , comments, , likes, fo...  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testsetdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "pipeline = Pipeline(stages=[stopwr, hashing, hashingIDF, gbt])\n",
    "\n",
    "pipeline = pipeline.fit(trainset.select(col('words'), col('label').cast('int')))\n",
    "\n",
    "\n",
    "predictions = pipeline.transform(testset.select(col('words'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4280,) (4280, 1)\n",
      "The accuracy of GBT Classifier to predict fake news is 63.88%\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.79      0.67      2020\n",
      "           1       0.73      0.50      0.59      2260\n",
      "\n",
      "    accuracy                           0.64      4280\n",
      "   macro avg       0.66      0.65      0.63      4280\n",
      "weighted avg       0.66      0.64      0.63      4280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "Yspark_pred = predictions.toPandas()\n",
    "Yspark_pred = Yspark_pred['prediction']\n",
    "\n",
    "ytestdf2 = testset.select('label').toPandas()\n",
    "\n",
    "print(Yspark_pred.shape, ytestdf2.shape)\n",
    "\n",
    "FK_accuracy = accuracy_score(ytestdf2, Yspark_pred) * 100\n",
    "FK_classification_report = classification_report(ytestdf2, Yspark_pred)\n",
    "\n",
    "print('The accuracy of GBT Classifier to predict fake news is {:.2f}%'.format(FK_accuracy))\n",
    "print('Classification Report: \\n', FK_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
