{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import glob, os\n",
    "# glob.glob(os.path.join(os.environ[\"SPARK_HOME\"], \"conf\", \"spark*\"))\n",
    "# print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "\n",
    "testdf = pd.read_csv('datasets/test.csv')\n",
    "# traindf = pd.read_csv('datasets/train.csv')\n",
    "alldf = pd.read_csv('datasets/submit.csv')\n",
    "\n",
    "# print(testdf.shape, traindf.shape)\n",
    "\n",
    "# print('cleaning dataframes')\n",
    "\n",
    "# print(np.sum(testdf.isnull()))\n",
    "\n",
    "testdf[['title', 'author']] = testdf[['title', 'author']].fillna(' ')\n",
    "\n",
    "testdf = testdf.dropna()\n",
    "# traindf = traindf.dropna()\n",
    "\n",
    "# print(testdf.shape, traindf.shape)\n",
    "\n",
    "ytestdf = testdf.drop(['title', 'author', 'text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5193, 2)\n"
     ]
    }
   ],
   "source": [
    "# ytestdf = ytestdf.merge(alldf, left_on='id', right_on='id', how='left')[['id', 'label']]\n",
    "ytestdf = pd.merge(left = ytestdf, right=alldf, left_on='id', right_on='id', how='left')\n",
    "print(ytestdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://Ryzen-PC:4040\n",
      "[('spark.app.name', 'fakenews'), ('spark.driver.host', 'Ryzen-PC'), ('spark.driver.memory', '10g'), ('spark.executor.id', 'driver'), ('spark.sql.warehouse.dir', 'file:/C:/Users/rushi/Documents/GitHub/fake-news-detection/spark-warehouse'), ('spark.app.id', 'local-1648561256887'), ('spark.app.startTime', '1648561256002'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.port', '58487')]\n",
      "<pyspark.sql.session.SparkSession object at 0x0000028E08925E50>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    master('local[*]').\\\n",
    "        config('spark.driver.memory', '10g').\\\n",
    "            appName('fakenews').\\\n",
    "                getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.uiWebUrl)\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "    # master('local[*]').\\\n",
    "    #     config('spark.driver.memory','10g').\\\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12256 5194\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "trainschema = StructType([StructField('id', IntegerType()),\\\n",
    "    StructField('title', StringType()),\\\n",
    "        StructField('author', StringType()),\\\n",
    "            StructField('text', StringType()),\\\n",
    "                StructField('label', IntegerType())]) \n",
    "\n",
    "testschema = StructType([StructField('id', IntegerType()),\\\n",
    "    StructField('title', StringType()),\\\n",
    "        StructField('author', StringType()),\\\n",
    "            StructField('text', StringType())]) \n",
    "\n",
    "trainset = spark.read.csv('datasets/train.csv', header=True, schema=trainschema)\n",
    "testset = spark.read.csv('datasets/test.csv', header=True, schema=testschema)\n",
    "\n",
    "\n",
    "\n",
    "trainset = trainset.fillna(' ', subset=['title', 'author']).dropna()\n",
    "testset = testset.fillna(' ', subset=['title', 'author']).dropna()\n",
    "\n",
    "print(trainset.count(), testset.count())\n",
    "\n",
    "# trainset.printSchema()\n",
    "# testset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+-----+\n",
      "| id|               title|              author|                text|label|\n",
      "+---+--------------------+--------------------+--------------------+-----+\n",
      "|  1|FLYNN: Hillary Cl...|     Daniel J. Flynn|Ever get the feel...|    0|\n",
      "|  5|Jackie Mason: Hol...|     Daniel Nussbaum|In these trying t...|    0|\n",
      "|  7|Benoît Hamon Wins...|     Alissa J. Rubin|PARIS  —   France...|    0|\n",
      "|  8|Excerpts From a D...|                 nan|Donald J. Trump i...|    0|\n",
      "|  9|A Back-Channel Pl...|Megan Twohey and ...|A week before Mic...|    0|\n",
      "| 10|Obama’s Organizin...|         Aaron Klein|Organizing for Ac...|    0|\n",
      "| 11|\"BBC Comedy Sketc...|     Chris Tomlinson|The BBC produced ...|    0|\n",
      "| 15|In Major League S...|       Jack Williams|Guillermo Barros ...|    0|\n",
      "| 16|Wells Fargo Chief...|Michael Corkery a...|The scandal engul...|    0|\n",
      "| 19|Chuck Todd: ’Buzz...|           Jeff Poor|Wednesday after  ...|    0|\n",
      "+---+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, CountVectorizer, StopWordsRemover, IDF\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex = '[,\\\\-]'\n",
    "\n",
    "trainset = trainset.withColumn('text', regexp_replace(trainset.text, regex, ' '))\n",
    "testset = testset.withColumn('text', regexp_replace(testset.text, regex, ' '))\n",
    "\n",
    "#tonkenize text\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "# wordsdata = tokenizer.transform(trainset)\n",
    "\n",
    "stopwr = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
    "# wordsdata = stopwr.transform(wordsdata)\n",
    "\n",
    "hashing = HashingTF(inputCol='filtered', outputCol='hashed', numFeatures=32)\n",
    "# wordsdata = hashing.transform(wordsdata)\n",
    "\n",
    "hashingIDF = IDF(inputCol='hashed', outputCol='features')\n",
    "# model = hashingIDF.fit(wordsdata)\n",
    "# wordsdata = model.transform(wordsdata)\n",
    "\n",
    "# vector = CountVectorizer(inputCol='filtered', outputCol='featuresCV')\n",
    "# model = vector.fit(wordsdata)\n",
    "\n",
    "# wordsdata = model.transform(wordsdata)\n",
    "#create a model\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(maxIter=1)\n",
    "# gbt.fit(wordsdata)\n",
    "\n",
    "# wordsdata.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwr, hashing, hashingIDF, gbt])\n",
    "\n",
    "pipeline = pipeline.fit(trainset.select(col('text'), col('label').cast('int')))\n",
    "\n",
    "\n",
    "predictions = pipeline.transform(testset.select(col('text'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5194,) (5193, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5193, 5194]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_67976/1543966843.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYspark_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytestdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mFK_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytestdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYspark_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mFK_classification_report\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytestdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYspark_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multilabel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5193, 5194]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "Yspark_pred = predictions.toPandas()\n",
    "Yspark_pred = Yspark_pred['prediction']\n",
    "\n",
    "print(Yspark_pred.shape, ytestdf.shape)\n",
    "\n",
    "FK_accuracy = accuracy_score(ytestdf, Yspark_pred) * 100\n",
    "FK_classification_report = classification_report(ytestdf, Yspark_pred)\n",
    "\n",
    "print('The accuracy of GBT Classifier to predict fake news is ', FK_accuracy)\n",
    "print('Classification Report: \\n', FK_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
